name: declare

services:
    kong:
        container_name: kong
        image: kong:latest
        environment:
            KONG_DATABASE: "off"
            KONG_DECLARATIVE_CONFIG: "/usr/local/kong/declarative/kong.yml"
            KONG_PROXY_ACCESS_LOG: "/dev/stdout"
            KONG_ADMIN_ACCESS_LOG: "/dev/stdout"
            KONG_PROXY_ERROR_LOG: "/dev/stderr"
            KONG_ADMIN_ERROR_LOG: "/dev/stderr"
            KONG_ADMIN_LISTEN: "0.0.0.0:8001"
            KONG_PROXY_LISTEN: "0.0.0.0:8000"
            KONG_PLUGINS: request-transformer,cors,key-auth,acl,basic-auth
            KONG_NGINX_PROXY_PROXY_BUFFER_SIZE: 160k
            KONG_NGINX_PROXY_PROXY_BUFFERS: 64 160k
            # example placeholders:
            ANON_KEY: ANON_KEY
            SERVICE_KEY: SERVICE_KEY
            DASHBOARD_USERNAME: DASHBOARD_USERNAME
            DASHBOARD_PASSWORD: DASHBOARD_PASSWORD
        ports:
            - "8000:8000"
            - "8001:8001"
        volumes:
            - ./kong/kong.yml:/usr/local/kong/declarative/kong.yml
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:8001/status"]
            interval: 10s
            timeout: 10s
            retries: 5
        restart: unless-stopped
    studio:
        container_name: studio
        build:
            context: ../
            dockerfile: __declare__/Dockerfile-studio
        environment:
            - DUMMY_JWT=${DUMMY_JWT}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:5173/"]
            interval: 1s
            timeout: 10s
            retries: 5
        ports:
            - "3000:3000"
        volumes:
            - ./studio:/usr/src/app
            - ./../models:/usr/src/app/mount/models
            - ./../workflows:/usr/src/app/mount/workflows
            - ./../dashboards:/usr/src/app/mount/dashboards
        restart: unless-stopped
        command: >
            /bin/sh -c "
                bun install && \
                bun run dev -- --host 0.0.0.0 --port 3000
            "
    studio-api:
        container_name: studio-api
        build:
            context: ../
            dockerfile: __declare__/Dockerfile-studio-api
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9998/"]
            interval: 1s
            timeout: 10s
            retries: 5
        ports:
            - "9998:9998"
        volumes:
            - ./studio-api:/usr/src/app
            - ./../models:/usr/src/app/mount/models
            - ./../workflows:/usr/src/app/mount/workflows
            - ./../dashboards:/usr/src/app/mount/dashboards
        restart: unless-stopped
    dashboards:
        container_name: dashboards
        build:
            context: ../
            dockerfile: __declare__/Dockerfile-dashboards
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:5173/"]
            interval: 1s
            timeout: 10s
            retries: 5
        ports:
            - "5173:5173"
        volumes:
            # Mount the dashboards source directory to the container for
            # watching
            - ./../dashboards:/usr/src/app/src/dashboards
            # Mount the model definitions directory to the container for
            # watching as well
            - ./../models:/usr/src/app/src/models
        restart: unless-stopped
    hot-model-reloader:
        container_name: hot-model-reloader
        build:
            context: ../
            dockerfile: __declare__/Dockerfile-hot-model-reloader
        depends_on:
            clickhouse:
                condition: service_healthy
        volumes:
            # Mount the model definitions directory to the container for
            # watching
            - ./../models:/usr/src/app/models
        restart: unless-stopped
    serve-models-api:
        container_name: serve-models-api
        build:
            context: ../
            dockerfile: __declare__/Dockerfile-serve-models-api
        ports:
            - "9002:9002"
        depends_on:
            clickhouse:
                condition: service_healthy
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9002/"]
            interval: 1s
            timeout: 10s
            retries: 5
        restart: unless-stopped
    sql-linter:
        container_name: sql-linter
        image: sqlfluff/sqlfluff:latest
        user: root # Run as root to gain necessary permissions
        volumes:
            - ./../models:/models
        entrypoint: >
            /bin/sh -c "
                # Run sqlfluff lint silently on all files initially
                sqlfluff lint /models --dialect clickhouse && \
                echo \"\" && \

                # Update and install inotify-tools quietly
                apt-get update -qq && \
                apt-get install -y --no-install-recommends inotify-tools > /dev/null 2>&1 && \

                # Clean up to reduce image size
                chmod -R 755 /var/lib/apt/lists && \
                rm -rf /var/lib/apt/lists/* && \

                # Start the inotifywait loop silently
                while true; do \
                    FILE=$$(inotifywait -e modify --format \"%w%f\" -r /models) > /dev/null 2>&1 && \
                    echo \"\" && \
                    sqlfluff lint \"$$FILE\" --dialect clickhouse; \
                    echo \"\" \
                done
            "
        restart: always
    workflow-worker:
        container_name: workflow-worker
        build:
            context: ../
            dockerfile: __declare__/Dockerfile-workflow-worker
        depends_on:
            hatchet-lite:
                condition: service_healthy
        # Use host networking to allow access to localhost:7077
        network_mode: "host"
        volumes:
            - ./../workflows:/usr/src/app/workflows
        restart: unless-stopped
    clickhouse:
        container_name: clickhouse
        image: clickhouse/clickhouse-server
        user: "101:101"
        hostname: clickhouse
        ports:
            # The HTTP api
            - "127.0.0.1:8123:8123"
            # The native protocol
            - "127.0.0.1:9000:9000"
        healthcheck:
            test: ["CMD", "wget", "--spider", "-q", "localhost:8123/ping"]
            interval: 30s
            timeout: 5s
            retries: 3
        restart: always
        security_opt:
            - seccomp:unconfined
    airflow-webserver:
        container_name: airflow-webserver
        image: apache/airflow:2.2.3
        environment:
            - AIRFLOW__CORE__EXECUTOR=LocalExecutor
            - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
            - AIRFLOW__CORE__FERNET_KEY=YOUR_FERNET_KEY
            - AIRFLOW__CORE__LOAD_EXAMPLES=False
        ports:
            - "8080:8080"
        volumes:
            - ./dags:/opt/airflow/dags
            - ./logs:/opt/airflow/logs
            - ./plugins:/opt/airflow/plugins
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
            interval: 10s
            timeout: 10s
            retries: 5
        restart: unless-stopped
    airflow-scheduler:
        container_name: airflow-scheduler
        image: apache/airflow:2.2.3
        environment:
            - AIRFLOW__CORE__EXECUTOR=LocalExecutor
            - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
            - AIRFLOW__CORE__FERNET_KEY=YOUR_FERNET_KEY
            - AIRFLOW__CORE__LOAD_EXAMPLES=False
        volumes:
            - ./dags:/opt/airflow/dags
            - ./logs:/opt/airflow/logs
            - ./plugins:/opt/airflow/plugins
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
            interval: 10s
            timeout: 10s
            retries: 5
        restart: unless-stopped
    airflow-worker:
        container_name: airflow-worker
        image: apache/airflow:2.2.3
        environment:
            - AIRFLOW__CORE__EXECUTOR=LocalExecutor
            - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
            - AIRFLOW__CORE__FERNET_KEY=YOUR_FERNET_KEY
            - AIRFLOW__CORE__LOAD_EXAMPLES=False
        volumes:
            - ./dags:/opt/airflow/dags
            - ./logs:/opt/airflow/logs
            - ./plugins:/opt/airflow/plugins
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
            interval: 10s
            timeout: 10s
            retries: 5
        restart: unless-stopped
    airflow-db:
        container_name: airflow-db
        image: postgres:13
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        ports:
            - "5432:5432"
        volumes:
            - airflow_db_data:/var/lib/postgresql/data
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
            interval: 1s
            timeout: 10s
            retries: 20
            start_period: 1s
        restart: unless-stopped
    redis:
        container_name: redis
        image: redis:6.2
        ports:
            - "6379:6379"
        volumes:
            - redis_data:/data
        healthcheck:
            test: ["CMD", "redis-cli", "ping"]
            interval: 10s
            timeout: 10s
            retries: 5
        restart: unless-stopped
    airflow-flower:
        container_name: airflow-flower
        image: apache/airflow:2.2.3
        environment:
            - AIRFLOW__CORE__EXECUTOR=LocalExecutor
            - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
            - AIRFLOW__CORE__FERNET_KEY=YOUR_FERNET_KEY
            - AIRFLOW__CORE__LOAD_EXAMPLES=False
        ports:
            - "5555:5555"
        volumes:
            - ./dags:/opt/airflow/dags
            - ./logs:/opt/airflow/logs
            - ./plugins:/opt/airflow/plugins
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:5555/health"]
            interval: 10s
            timeout: 10s
            retries: 5
        restart: unless-stopped

volumes:
    airflow_db_data:
    redis_data:
