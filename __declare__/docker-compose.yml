name: declare

# _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all airflow containers.
#                                Use this option ONLY for quick checks. Installing requirements at container
#                                startup is done EVERY TIME the service is started.
#                                A better way is to build a custom image or extend the official image
#                                as described in https://airflow.apache.org/docs/docker-stack/build.html.
#                                Default: ''
x-airflow-common: &airflow-common
    image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.10.2}
    environment: &airflow-common-env
        AIRFLOW__CORE__EXECUTOR: CeleryExecutor
        AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@declare-workflow-db/airflow
        AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@declare-workflow-db/airflow
        AIRFLOW__CELERY__BROKER_URL: redis://:@declare-workflow-cache:6379/0
        AIRFLOW__CORE__FERNET_KEY: ""
        AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
        AIRFLOW__CORE__LOAD_EXAMPLES: "true"
        AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
        # yamllint disable rule:line-length
        # Use simple http server on scheduler for health checks
        # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
        # yamllint enable rule:line-length
        AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "true"
        # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
        # for other purpose (development, test and especially production usage) build/extend Airflow image.
        _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
        # The following line can be used to set a custom config file, stored in the local config folder
        # If you want to use it, outcomment it and replace airflow.cfg with the name of your config file
        # AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
    volumes:
        - ${AIRFLOW_PROJ_DIR:-.}/workflows:/opt/airflow/dags
        # - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
        # - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
        # - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
    user: "${AIRFLOW_UID:-50000}:0"
    depends_on: &airflow-common-depends-on
        declare-workflow-cache:
            condition: service_healthy
        declare-workflow-db:
            condition: service_healthy

services:
    declare-gateway:
        container_name: declare-gateway
        image: kong:latest
        environment:
            KONG_DATABASE: "off"
            KONG_DECLARATIVE_CONFIG: "/usr/local/kong/declarative/kong.yml"
            KONG_PROXY_ACCESS_LOG: "/dev/stdout"
            KONG_ADMIN_ACCESS_LOG: "/dev/stdout"
            KONG_PROXY_ERROR_LOG: "/dev/stderr"
            KONG_ADMIN_ERROR_LOG: "/dev/stderr"
            KONG_ADMIN_LISTEN: "0.0.0.0:8001"
            KONG_PROXY_LISTEN: "0.0.0.0:8000"
            KONG_PLUGINS: request-transformer,cors,key-auth,acl,basic-auth
            KONG_NGINX_PROXY_PROXY_BUFFER_SIZE: 160k
            KONG_NGINX_PROXY_PROXY_BUFFERS: 64 160k
            # example placeholders:
            ANON_KEY: ANON_KEY
            SERVICE_KEY: SERVICE_KEY
            DASHBOARD_USERNAME: DASHBOARD_USERNAME
            DASHBOARD_PASSWORD: DASHBOARD_PASSWORD
        ports:
            - "8000:8000"
            - "8001:8001"
        volumes:
            - ./kong/kong.yml:/usr/local/kong/declarative/kong.yml
        healthcheck:
            test: ["CMD", "kong", "health"]
            interval: 1s
            timeout: 10s
            retries: 5
        restart: unless-stopped
    declare-studio:
        container_name: declare-studio
        build:
            context: ../
            dockerfile: __declare__/Dockerfile-studio
        environment:
            - DUMMY_JWT=${DUMMY_JWT}
        healthcheck:
            test:
                [
                    "CMD",
                    "bun",
                    "-e",
                    "require('http').get('http://localhost:3000', (r) => {if (r.statusCode !== 200) throw new Error(r.statusCode)})",
                ]
            interval: 1s
            timeout: 10s
            retries: 5
        ports:
            - "3000:3000"
        volumes:
            - ./studio:/usr/src/app
            - ./../models:/usr/src/app/mount/models
            - ./../workflows:/usr/src/app/mount/workflows
            - ./../dashboards:/usr/src/app/mount/dashboards
        restart: unless-stopped
        command: >
            /bin/sh -c "
                bun install && \
                bun run dev -- --host 0.0.0.0 --port 3000
            "
    declare-api:
        container_name: declare-api
        build:
            context: ../
            dockerfile: __declare__/Dockerfile-studio-api
        healthcheck:
            test:
                [
                    "CMD",
                    "bun",
                    "-e",
                    "require('http').get('http://localhost:9998/status', (r) => {if (r.statusCode !== 200) throw new Error(r.statusCode)})",
                ]
            interval: 1s
            timeout: 10s
            retries: 5
        ports:
            - "9998:9998"
        volumes:
            - ./studio-api:/usr/src/app
            - ./../models:/usr/src/app/mount/models
            - ./../workflows:/usr/src/app/mount/workflows
            - ./../dashboards:/usr/src/app/mount/dashboards
        restart: unless-stopped
    declare-dashboards:
        container_name: declare-dashboards
        build:
            context: ../
            dockerfile: __declare__/Dockerfile-dashboards
        healthcheck:
            test:
                [
                    "CMD",
                    "node",
                    "-e",
                    "require('http').get('http://localhost:5173', (r) => {if (r.statusCode !== 200) throw new Error(r.statusCode)})",
                ]
            interval: 1s
            timeout: 10s
            retries: 5
        ports:
            - "5173:5173"
        volumes:
            # Mount the dashboards source directory to the container for
            # watching
            - ./../dashboards:/usr/src/app/src/dashboards
            # Mount the model definitions directory to the container for
            # watching as well
            - ./../models:/usr/src/app/src/models
        restart: unless-stopped
    declare-hot-model-reloader:
        container_name: declare-hot-model-reloader
        build:
            context: ../
            dockerfile: __declare__/Dockerfile-hot-model-reloader
        depends_on:
            declare-warehouse-clickhouse:
                condition: service_healthy
        volumes:
            # Mount the model definitions directory to the container for
            # watching
            - ./../models:/usr/src/app/models
        restart: unless-stopped
    declare-serve-models-api:
        container_name: declare-serve-models-api
        build:
            context: ../
            dockerfile: __declare__/Dockerfile-serve-models-api
        ports:
            - "9002:9002"
        depends_on:
            declare-warehouse-clickhouse:
                condition: service_healthy
        healthcheck:
            test:
                [
                    "CMD",
                    "bun",
                    "-e",
                    "require('http').get('http://localhost:9002', (r) => {if (r.statusCode !== 200) throw new Error(r.statusCode)})",
                ]
            interval: 1s
            timeout: 10s
            retries: 5
        restart: unless-stopped
    declare-sql-linter:
        container_name: declare-sql-linter
        image: sqlfluff/sqlfluff:latest
        user: root # Run as root to gain necessary permissions
        volumes:
            - ./../models:/models
        entrypoint: >
            /bin/sh -c "
                # Run sqlfluff lint silently on all files initially
                sqlfluff lint /models --dialect clickhouse && \
                echo \"\" && \

                # Update and install inotify-tools quietly
                apt-get update -qq && \
                apt-get install -y --no-install-recommends inotify-tools > /dev/null 2>&1 && \

                # Clean up to reduce image size
                chmod -R 755 /var/lib/apt/lists && \
                rm -rf /var/lib/apt/lists/* && \

                # Start the inotifywait loop silently
                while true; do \
                    FILE=$$(inotifywait -e modify --format \"%w%f\" -r /models) > /dev/null 2>&1 && \
                    echo \"\" && \
                    sqlfluff lint \"$$FILE\" --dialect clickhouse; \
                    echo \"\" \
                done
            "
        restart: always
    declare-warehouse-clickhouse:
        container_name: declare-warehouse-clickhouse
        image: clickhouse/clickhouse-server
        user: "101:101"
        hostname: clickhouse
        ports:
            # The HTTP api
            - "127.0.0.1:8123:8123"
            # The native protocol
            - "127.0.0.1:9000:9000"
        healthcheck:
            test: ["CMD", "wget", "--spider", "-q", "localhost:8123/ping"]
            interval: 30s
            timeout: 5s
            retries: 3
        restart: always
        security_opt:
            - seccomp:unconfined

    # Airflow
    declare-workflow-db:
        container_name: declare-workflow-postgres
        image: postgres:13
        environment:
            POSTGRES_USER: airflow
            POSTGRES_PASSWORD: airflow
            POSTGRES_DB: airflow
        # Let's have it be stateless for now
        # volumes:
        #     - postgres-db-volume:/var/lib/postgresql/data
        healthcheck:
            test: ["CMD", "pg_isready", "-U", "airflow"]
            interval: 10s
            retries: 5
            start_period: 5s
        restart: always

    declare-workflow-cache:
        container_name: declare-workflow-cache
        # Redis is limited to 7.2-bookworm due to licencing change
        # https://redis.io/blog/redis-adopts-dual-source-available-licensing/
        image: redis:7.2-bookworm
        expose:
            - 6379
        healthcheck:
            test: ["CMD", "redis-cli", "ping"]
            interval: 10s
            timeout: 30s
            retries: 50
            start_period: 30s
        restart: always

    declare-workflow-web-ui:
        <<: *airflow-common
        container_name: declare-workflow-web-ui
        command: webserver
        ports:
            - "8080:8080"
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        restart: always
        depends_on:
            <<: *airflow-common-depends-on
            declare-workflow-init-checks:
                condition: service_completed_successfully

    declare-workflow-scheduler:
        <<: *airflow-common
        container_name: declare-workflow-scheduler
        command: scheduler
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        restart: always
        depends_on:
            <<: *airflow-common-depends-on
            declare-workflow-init-checks:
                condition: service_completed_successfully

    declare-workflow-worker:
        <<: *airflow-common
        container_name: declare-workflow-worker
        command: celery worker
        healthcheck:
            # yamllint disable rule:line-length
            test:
                - "CMD-SHELL"
                - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        environment:
            <<: *airflow-common-env
            # Required to handle warm shutdown of the celery workers properly
            # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
            DUMB_INIT_SETSID: "0"
        restart: always
        depends_on:
            <<: *airflow-common-depends-on
            declare-workflow-init-checks:
                condition: service_completed_successfully

    declare-workflow-triggerer:
        <<: *airflow-common
        container_name: declare-workflow-triggerer
        command: triggerer
        healthcheck:
            test:
                [
                    "CMD-SHELL",
                    'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"',
                ]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        restart: always
        depends_on:
            <<: *airflow-common-depends-on
            declare-workflow-init-checks:
                condition: service_completed_successfully

    declare-workflow-init-checks:
        <<: *airflow-common
        container_name: declare-workflow-init-checks
        entrypoint: /bin/bash
        # yamllint disable rule:line-length
        command:
            - -c
            - |
                if [[ -z "${AIRFLOW_UID}" ]]; then
                  echo
                  echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
                  echo "If you are on Linux, you SHOULD follow the instructions below to set "
                  echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
                  echo "For other operating systems you can get rid of the warning with manually created .env file:"
                  echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
                  echo
                fi
                one_meg=1048576
                mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
                cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
                disk_available=$$(df / | tail -1 | awk '{print $$4}')
                warning_resources="false"
                if (( mem_available < 4000 )) ; then
                  echo
                  echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
                  echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
                  echo
                  warning_resources="true"
                fi
                if (( cpus_available < 2 )); then
                  echo
                  echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
                  echo "At least 2 CPUs recommended. You have $${cpus_available}"
                  echo
                  warning_resources="true"
                fi
                if (( disk_available < one_meg * 10 )); then
                  echo
                  echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
                  echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
                  echo
                  warning_resources="true"
                fi
                if [[ $${warning_resources} == "true" ]]; then
                  echo
                  echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
                  echo "Please follow the instructions to increase amount of resources available:"
                  echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
                  echo
                fi
                mkdir -p /sources/logs /sources/dags /sources/plugins
                chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
                exec /entrypoint airflow version
        # yamllint enable rule:line-length
        environment:
            <<: *airflow-common-env
            _AIRFLOW_DB_MIGRATE: "true"
            _AIRFLOW_WWW_USER_CREATE: "true"
            _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
            _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
            _PIP_ADDITIONAL_REQUIREMENTS: ""
        user: "0:0"
        # volumes:
        #     - ${AIRFLOW_PROJ_DIR:-.}:/sources

    # airflow-cli:
    # container_name: declare-workflow-cli
    #     <<: *airflow-common
    #     profiles:
    #         - debug
    #     environment:
    #         <<: *airflow-common-env
    #         CONNECTION_CHECK_MAX_COUNT: "0"
    #     # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
    #     command:
    #         - bash
    #         - -c
    #         - airflow

    # You can enable flower by adding "--profile flower" option e.g. docker-compose --profile flower up
    # or by explicitly targeted on the command line e.g. docker-compose up flower.
    # See: https://docs.docker.com/compose/profiles/
    declare-workflow-monitoring:
        <<: *airflow-common
        container_name: declare-workflow-monitoring
        command: celery flower
        # profiles:
        #     - flower
        ports:
            - "5555:5555"
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        restart: always
        depends_on:
            <<: *airflow-common-depends-on
            declare-workflow-init-checks:
                condition: service_completed_successfully
    declare-ready-notifier:
        container_name: declare-ready-notifier
        # This service prints a message to the console when every container is ready
        image: busybox:latest
        entrypoint: /bin/sh
        command: -c "echo 'All containers are ready!'"
        depends_on:
            declare-gateway:
                condition: service_healthy
            declare-studio:
                condition: service_healthy
            declare-api:
                condition: service_healthy
            declare-dashboards:
                condition: service_healthy
            declare-hot-model-reloader:
                condition: service_started
            declare-serve-models-api:
                condition: service_healthy
            declare-sql-linter:
                condition: service_started
            declare-warehouse-clickhouse:
                condition: service_healthy
            declare-workflow-db:
                condition: service_healthy
            declare-workflow-cache:
                condition: service_healthy
            declare-workflow-web-ui:
                condition: service_healthy
            declare-workflow-scheduler:
                condition: service_healthy
            declare-workflow-worker:
                condition: service_healthy
            declare-workflow-triggerer:
                condition: service_healthy
            declare-workflow-init-checks:
                condition: service_completed_successfully
            declare-workflow-monitoring:
                condition: service_healthy
        restart: "no"

volumes:
    postgres-db-volume:
